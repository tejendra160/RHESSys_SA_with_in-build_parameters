{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed42c0-cebd-4962-a5ca-30bb658c77bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the below code will generate the number of the def files and the morris random parameters values with OAT rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460d56d-6944-410a-be16-0e3a95d75730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d34a3e-7b86-4d5a-be0c-90dd6cea2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SALib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8472c-acd1-4437-b9dc-4bf810abaa78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, shutil, numpy as np, re, pandas as pd\n",
    "from SALib.sample import morris\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "SOURCE_DEFS_DIR = os.path.join(BASE_DIR, \"defs\")\n",
    "SA_DEFS_PARENT = os.path.join(BASE_DIR, \"SA_defs_morris\")\n",
    "os.makedirs(SA_DEFS_PARENT, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 2025\n",
    "NUM_TRAJECTORIES = 100   # Set higher for real runs!\n",
    "NUM_LEVELS = 10\n",
    "\n",
    "unchanged_files = set([\n",
    "    \"basin_basin.def\", \"hill_setting1.def\", \"landuse_setting5.def\",\n",
    "    \"stratum_crop.def\", \"stratum_grass.def\", \"stratum_shrub.def\"\n",
    "])\n",
    "\n",
    "files_to_change = [f for f in os.listdir(SOURCE_DEFS_DIR)\n",
    "                   if f.endswith(\".def\") and f not in unchanged_files]\n",
    "\n",
    "SUM_ONE_GROUPS = {\n",
    "    \"soil_loam_9.def\": [('clay', 'sand', 'silt')],\n",
    "    \"soil_sand_10.def\": [('clay', 'sand', 'silt')],\n",
    "    \"soil_sandy_loam_12.def\": [('clay', 'sand', 'silt')],\n",
    "    \"soil_silt_loam_8.def\": [('clay', 'sand', 'silt')],\n",
    "    \"soil_silty_clay_loam_3.def\": [('clay', 'sand', 'silt')],\n",
    "    \"stratum_cwt_rhododendron_bgc.def\": [\n",
    "        ('epc.leaflitr_fcel','epc.leaflitr_flab','epc.leaflitr_flig'),\n",
    "        ('epc.frootlitr_fcel','epc.frootlitr_flab','epc.frootlitr_flig'),\n",
    "        ('epc.deadwood_fcel','epc.deadwood_flig')],\n",
    "    \"stratum_deciduous.def\": [\n",
    "        ('epc.leaflitr_fcel','epc.leaflitr_flab','epc.leaflitr_flig'),\n",
    "        ('epc.frootlitr_fcel','epc.frootlitr_flab','epc.frootlitr_flig'),\n",
    "        ('epc.deadwood_fcel','epc.deadwood_flig')],\n",
    "    \"stratum_eastern_white_pine.def\": [\n",
    "        ('epc.leaflitr_fcel','epc.leaflitr_flab','epc.leaflitr_flig'),\n",
    "        ('epc.frootlitr_fcel','epc.frootlitr_flab','epc.frootlitr_flig'),\n",
    "        ('epc.deadwood_fcel','epc.deadwood_flig')],\n",
    "    \"stratum_evergreen.def\": [\n",
    "        ('epc.leaflitr_fcel','epc.leaflitr_flab','epc.leaflitr_flig'),\n",
    "        ('epc.frootlitr_fcel','epc.frootlitr_flab','epc.frootlitr_flig'),\n",
    "        ('epc.deadwood_fcel','epc.deadwood_flig')],\n",
    "    \"stratum_localdeciduous.def\": [\n",
    "        ('epc.leaflitr_fcel','epc.leaflitr_flab','epc.leaflitr_flig'),\n",
    "        ('epc.frootlitr_fcel','epc.frootlitr_flab','epc.frootlitr_flig'),\n",
    "        ('epc.deadwood_fcel','epc.deadwood_flig')]\n",
    "}\n",
    "\n",
    "DAY_INT_PARAMS = {\"epc.day_leafoff\", \"epc.day_leafon\", \"epc.ndays_expand\", \"epc.ndays_litfall\"}\n",
    "\n",
    "MULT1000_PARAMS = {\n",
    "    \"epc.vpd_close (x1000)\",\n",
    "    \"epc.vpd_open (x1000)\",\n",
    "    \"gsurf_intercept\",\n",
    "    \"snow_light_ext_coef\"\n",
    "}\n",
    "\n",
    "CN_RANGES = {\"broadleaf\": (20, 50, 45, 98),\n",
    "             \"pine\": (20, 50, 45, 98),\n",
    "             \"alder\": (20, 50, 45, 98),\n",
    "             \"grass\": (20, 50, 45, 98)}\n",
    "\n",
    "SOIL_PREFIXES = [\n",
    "    \"soil_loam_9\",\n",
    "    \"soil_sand_10\",\n",
    "    \"soil_sandy_loam_12\",\n",
    "    \"soil_silt_loam_8\",\n",
    "    \"soil_silty_clay_loam_3\"\n",
    "]\n",
    "KSAT0 = \"Ksat_0\"\n",
    "KSAT0_V = \"Ksat_0_v\"\n",
    "\n",
    "LAPSE_ZONE_PREFIX = \"zone_setting2010\"\n",
    "LAPSE_TMAX = \"lapse_rate_tmax (= lapse rate)\"\n",
    "LAPSE_TMIN = \"lapse_rate_tmin (= lapse rate)\"\n",
    "\n",
    "def detect_veg_type(fn):\n",
    "    fn = fn.lower()\n",
    "    if \"pine\" in fn or \"evergreen\" in fn: return \"pine\"\n",
    "    elif \"alder\" in fn: return \"alder\"\n",
    "    elif \"grass\" in fn: return \"grass\"\n",
    "    return \"broadleaf\"\n",
    "\n",
    "def enforce_sum_to_one(params, file_prefix):\n",
    "    for grp in SUM_ONE_GROUPS.get(file_prefix + \".def\", []):\n",
    "        keys = [f\"{file_prefix}_{p}\" for p in grp]\n",
    "        vals = [max(params.get(k, 0), 0) for k in keys]\n",
    "        s = sum(vals)\n",
    "        if s == 0:\n",
    "            vals = [1.0 / len(grp)] * len(grp)\n",
    "        else:\n",
    "            vals = [v / s for v in vals]\n",
    "        decimals = 8\n",
    "        vals = [round(v, decimals) for v in vals[:-1]]\n",
    "        last_val = round(1.0 - sum(vals), decimals)\n",
    "        vals.append(last_val)\n",
    "        total = sum(vals)\n",
    "        if abs(total-1.0) > 1e-8:\n",
    "            vals = [round(v + (1.0-total)/len(vals), decimals) for v in vals]\n",
    "        for k, v in zip(keys, vals):\n",
    "            params[k] = v\n",
    "    return params\n",
    "\n",
    "def enforce_cn(params, file_prefix):\n",
    "    veg = detect_veg_type(file_prefix)\n",
    "    leaf_min, leaf_max, lit_min, lit_max = CN_RANGES[veg]\n",
    "    leaf_cn_key = f\"{file_prefix}_epc.leaf_cn\"\n",
    "    leaflitr_cn_key = f\"{file_prefix}_epc.leaflitr_cn\"\n",
    "    if leaf_cn_key in params:\n",
    "        params[leaf_cn_key] = np.clip(params[leaf_cn_key], leaf_min, leaf_max)\n",
    "    if leaflitr_cn_key in params:\n",
    "        min_lit = max(lit_min, params.get(leaf_cn_key, leaf_min)+5)\n",
    "        params[leaflitr_cn_key] = np.clip(params[leaflitr_cn_key], min_lit, lit_max)\n",
    "    return params\n",
    "\n",
    "def enforce_days(params, file_prefix):\n",
    "    for d in DAY_INT_PARAMS:\n",
    "        key = f\"{file_prefix}_{d}\"\n",
    "        if key in params:\n",
    "            params[key] = int(round(min(365,max(0,params[key]))))\n",
    "    l_on = f\"{file_prefix}_epc.day_leafon\"\n",
    "    l_off = f\"{file_prefix}_epc.day_leafoff\"\n",
    "    if l_on in params and l_off in params:\n",
    "        if params[l_on] >= params[l_off]:\n",
    "            params[l_on] = min(params[l_on], 180)\n",
    "            params[l_off] = max(params[l_on]+30, params[l_off])\n",
    "            params[l_off] = min(365, params[l_off])\n",
    "    return params\n",
    "\n",
    "def enforce_multiple_1000(params):\n",
    "    for k in params:\n",
    "        for pname in MULT1000_PARAMS:\n",
    "            if k.endswith(\"_\" + pname) or k == pname:\n",
    "                params[k] = int(np.floor(params[k] / 1000)*1000)\n",
    "    return params\n",
    "\n",
    "def unify_ksat_pairs(params):\n",
    "    \"\"\"\n",
    "    For every soil, ensure soil_xxx_Ksat_0 == soil_xxx_Ksat_0_v, always.\n",
    "    \"\"\"\n",
    "    for prefix in SOIL_PREFIXES:\n",
    "        ksat0 = f\"{prefix}_{KSAT0}\"\n",
    "        ksat0v = f\"{prefix}_{KSAT0_V}\"\n",
    "        if ksat0 in params:\n",
    "            params[ksat0v] = params[ksat0]\n",
    "        elif ksat0v in params:\n",
    "            params[ksat0] = params[ksat0v]\n",
    "    return params\n",
    "\n",
    "def unify_zone_lapse(params):\n",
    "    \"\"\"\n",
    "    For each parameter set, set tmin lapse to exactly match tmax lapse.\n",
    "    \"\"\"\n",
    "    tmax_key = f\"{LAPSE_ZONE_PREFIX}_{LAPSE_TMAX}\"\n",
    "    tmin_key = f\"{LAPSE_ZONE_PREFIX}_{LAPSE_TMIN}\"\n",
    "    if tmax_key in params:\n",
    "        params[tmin_key] = params[tmax_key]\n",
    "    elif tmin_key in params:\n",
    "        params[tmax_key] = params[tmin_key]\n",
    "    return params\n",
    "\n",
    "def enforce_all_constraints(params):\n",
    "    for f in files_to_change:\n",
    "        file_prefix = f.replace('.def','')\n",
    "        params = enforce_sum_to_one(params, file_prefix)\n",
    "        params = enforce_cn(params, file_prefix)\n",
    "        params = enforce_days(params, file_prefix)\n",
    "    params = enforce_multiple_1000(params)\n",
    "    params = unify_ksat_pairs(params)\n",
    "    params = unify_zone_lapse(params)\n",
    "    return params\n",
    "\n",
    "def parse_param_line(line):\n",
    "    m = re.match(r'^\\s*([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)[ \\t]+([a-zA-Z0-9_\\.]+(?: \\(= lapse rate\\))?)', line)\n",
    "    return (float(m.group(1)), m.group(2), line[m.end():]) if m else (None, None, None)\n",
    "\n",
    "def is_integer_param(file_prefix, param_name):\n",
    "    if param_name in DAY_INT_PARAMS:\n",
    "        return True\n",
    "    if param_name in MULT1000_PARAMS:\n",
    "        return True\n",
    "    full_key = f\"{file_prefix}_{param_name}\"\n",
    "    return any([full_key.endswith(\"_\" + p) for p in DAY_INT_PARAMS | MULT1000_PARAMS])\n",
    "\n",
    "# ====== LOAD PARAMETER RANGES & GENERATE SAMPLES\n",
    "bounds = pd.read_csv(os.path.join(BASE_DIR, \"Parameters_range_values.csv\"))\n",
    "param_names, bnds = [], []\n",
    "for _, r in bounds.iterrows():\n",
    "    try:\n",
    "        lo, hi = float(r['lower limit']), float(r['upper limit'])\n",
    "        if lo < hi:\n",
    "            param_names.append(r['Parameter name'])\n",
    "            bnds.append([lo, hi])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "problem = {'num_vars': len(param_names), 'names': param_names, 'bounds': bnds}\n",
    "X = morris.sample(problem, N=NUM_TRAJECTORIES, num_levels=NUM_LEVELS, seed=RANDOM_SEED)\n",
    "raw_df = pd.DataFrame(X, columns=param_names)\n",
    "\n",
    "# ====== ADJUSTED SAMPLES AND CONSTRAINTS\n",
    "adjusted_samples = []\n",
    "for idx, row in raw_df.iterrows():\n",
    "    params = row.to_dict()\n",
    "    params = enforce_all_constraints(params)\n",
    "    for k, v in params.items():\n",
    "        if \"_\" in k:\n",
    "            prefix, param = k.rsplit(\"_\", 1)\n",
    "            if is_integer_param(prefix, param):\n",
    "                if param in MULT1000_PARAMS:\n",
    "                    params[k] = int(np.floor(v / 1000)*1000)\n",
    "                else:\n",
    "                    params[k] = int(round(v))\n",
    "            elif isinstance(v, float):\n",
    "                params[k] = round(v, 8)\n",
    "        elif isinstance(v, float):\n",
    "            params[k] = round(v, 8)\n",
    "    # Again, enforce both constraints after rounding/typecasting\n",
    "    params = unify_ksat_pairs(params)\n",
    "    params = unify_zone_lapse(params)\n",
    "    adjusted_samples.append(params)\n",
    "\n",
    "adjusted_df = pd.DataFrame(adjusted_samples, columns=param_names)\n",
    "adjusted_df.insert(0, \"defs_set\", [f\"defs{i+1}\" for i in range(len(adjusted_df))])\n",
    "adjusted_df.to_csv(os.path.join(BASE_DIR, \"defs_parameter_mapping.csv\"), index=False)\n",
    "adjusted_df.drop(\"defs_set\", axis=1).to_csv(os.path.join(BASE_DIR, \"morris_parameter_set_full.csv\"), index=False)\n",
    "\n",
    "# ====== WRITE DEFS FILES FOR EACH SAMPLE\n",
    "def write_defs(sample_params, idx):\n",
    "    defsdir = os.path.join(SA_DEFS_PARENT, f\"defs{idx}\")\n",
    "    os.makedirs(defsdir, exist_ok=True)\n",
    "    for f in unchanged_files:\n",
    "        shutil.copy2(os.path.join(SOURCE_DEFS_DIR, f), os.path.join(defsdir, f))\n",
    "    for f in files_to_change:\n",
    "        file_prefix = f.replace('.def','')\n",
    "        lines = open(os.path.join(SOURCE_DEFS_DIR, f)).read().splitlines(True)\n",
    "        out = []\n",
    "        for ln in lines:\n",
    "            val, p, rest = parse_param_line(ln)\n",
    "            if not p:\n",
    "                out.append(ln)\n",
    "                continue\n",
    "            if p.endswith(\"default_ID\"):\n",
    "                out.append(ln)\n",
    "                continue\n",
    "            k = f\"{file_prefix}_{p}\"\n",
    "            if k in sample_params:\n",
    "                nv = sample_params[k]\n",
    "                if is_integer_param(file_prefix, p):\n",
    "                    if p in MULT1000_PARAMS:\n",
    "                        nv = int(np.floor(nv / 1000) * 1000)\n",
    "                    out.append(f\"{int(nv)} {p}{rest}\")\n",
    "                elif isinstance(nv, float) and float(nv).is_integer():\n",
    "                    out.append(f\"{int(nv)} {p}{rest}\")\n",
    "                else:\n",
    "                    out.append(f\"{nv:.8f} {p}{rest}\")\n",
    "            else:\n",
    "                out.append(ln)\n",
    "        open(os.path.join(defsdir, f), 'w').writelines(out)\n",
    "    return defsdir\n",
    "\n",
    "maprec = []\n",
    "for i, row in adjusted_df.iterrows():\n",
    "    defsdir = write_defs(row.to_dict(), i + 1)\n",
    "    maprec.append(row.to_dict())\n",
    "    maprec[-1]['defs_set'] = f\"defs{i+1}\"\n",
    "    print(\"Created:\", defsdir)\n",
    "\n",
    "NUM_DEFS_FOLDERS = len(adjusted_df)\n",
    "\n",
    "pd.DataFrame(maprec).to_csv(os.path.join(BASE_DIR, \"defs_parameter_mapping.csv\"), index=False)\n",
    "print(\"Mapping file written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a9f06-d885-4272-937f-74a611c618e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca4d98-6b4d-442c-b0b9-d172e10cfc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1bc5a-a881-40f0-ae91-cab2b3f50f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "SA_DEFS_PARENT = os.path.join(BASE_DIR, \"SA_defs_morris\")\n",
    "WORLDFILES_DIR = os.path.join(BASE_DIR, \"worldfiles\")\n",
    "os.makedirs(WORLDFILES_DIR, exist_ok=True)\n",
    "\n",
    "WORLD_HEADER_TEMPLATE = [\n",
    "    \"1 num_basin_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/basin_basin.def basin_default_filename\\n\",\n",
    "    \"1 num_hillslope_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/hill_setting1.def hillslope_default_filename\\n\",\n",
    "    \"1 num_zone_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/zone_setting2010.def zone_default_filename\\n\",\n",
    "    \"5 num_patch_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/soil_loam_9.def patch_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/soil_sand_10.def patch_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/soil_sandy_loam_12.def patch_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/soil_silt_loam_8.def patch_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/soil_silty_clay_loam_3.def patch_default_filename\\n\",\n",
    "    \"1 num_landuse_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/landuse_setting5.def landuse_default_filename\\n\",\n",
    "    \"8 num_stratum_files\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_localdeciduous.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_evergreen.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_crop.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_cwt_rhododendron_bgc.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_deciduous.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_eastern_white_pine.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_grass.def stratum_default_filename\\n\",\n",
    "    \"SA_defs_morris/defs{num}/stratum_shrub.def stratum_default_filename\\n\",\n",
    "    \"1 num_base_stations\\n\",\n",
    "    \"clim/W22 base_station_filename\\n\"\n",
    "]\n",
    "\n",
    "def create_worldfiles(num_files):\n",
    "    os.makedirs(WORLDFILES_DIR, exist_ok=True)\n",
    "    for i in range(1, num_files + 1):\n",
    "        worldfile_name = f\"worldfile{i}.hdr\"\n",
    "        worldfile_path = os.path.join(WORLDFILES_DIR, worldfile_name)\n",
    "        with open(worldfile_path, 'w') as wf:\n",
    "            for line in WORLD_HEADER_TEMPLATE:\n",
    "                wf.write(line.format(num=i))\n",
    "    print(f\"Generated {num_files} worldfile.hdr files in {WORLDFILES_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_worldfiles(NUM_DEFS_FOLDERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7396f98-3285-4d0a-b6af-e5909a4b2645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1521ce-faff-48c5-b998-9dd0ebbab8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DEFS_FOLDERS = len(adjusted_df)\n",
    "NUM_DEFS_FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbc072-f132-4e4e-9091-676019d03f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25e84c-ed01-454a-a30d-be4d5a1ffdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "RHESSYS_EXE = \"./RHESSysEastCoast/rhessysEC.7.2-2025-08-12-15-50-33\"\n",
    "WORLDHDR_DIR = \"worldfiles\"\n",
    "OUTPUT_DIR = \"SA_output_basin\"\n",
    "JOB_DIR = os.path.join(BASE_DIR, \"SA_job\")\n",
    "\n",
    "SLURM_TIME = \"90:00:00\"\n",
    "START_DATE = (1930, 1, 1, 1)\n",
    "END_DATE = (2020, 12, 30, 1)\n",
    "\n",
    "PARAM_CSV = os.path.join(BASE_DIR, \"defs_parameter_mapping.csv\")\n",
    "\n",
    "GW_NAMES = ['gw1', 'gw2', 'gw3']\n",
    "S_NAMES = ['s1', 's2', 's3']\n",
    "SV_NAMES = ['sv1', 'sv2']\n",
    "SVALT_NAMES = ['svalt1', 'svalt2']\n",
    "\n",
    "def main():\n",
    "    os.makedirs(JOB_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_DIR, OUTPUT_DIR), exist_ok=True)\n",
    "    st_year, st_month, st_day, st_hour = START_DATE\n",
    "    ed_year, ed_month, ed_day, ed_hour = END_DATE\n",
    "\n",
    "    param_df = pd.read_csv(PARAM_CSV)\n",
    "    NUM_DEFS_FOLDERS = len(param_df)\n",
    "\n",
    "    for i in range(NUM_DEFS_FOLDERS):\n",
    "        row = param_df.iloc[i]\n",
    "        gw_vals = [row[name] for name in GW_NAMES]\n",
    "        s_vals = [row[name] for name in S_NAMES]\n",
    "        sv_vals = [row[name] for name in SV_NAMES]\n",
    "        svalt_vals = [row[name] for name in SVALT_NAMES]\n",
    "\n",
    "        param_str = (\n",
    "            f\"-gw {' '.join(map(str, gw_vals))} \"\n",
    "            f\"-s {' '.join(map(str, s_vals))} \"\n",
    "            f\"-sv {' '.join(map(str, sv_vals))} \"\n",
    "            f\"-svalt {' '.join(map(str, svalt_vals))}\"\n",
    "        )\n",
    "\n",
    "        worldfile_hdr = f\"{WORLDHDR_DIR}/worldfile{i+1}.hdr\"\n",
    "        output_prefix = f\"{OUTPUT_DIR}/Salyan{i+1}\"\n",
    "        job_path = os.path.join(JOB_DIR, f\"basin{i+1}.job\")\n",
    "        jobname = f\"basin{i+1}\"\n",
    "        job_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem=90gb\n",
    "#SBATCH -t {SLURM_TIME}\n",
    "#SBATCH --output=/dev/null\n",
    "#SBATCH --error=basin_rosi.err\n",
    "#SBATCH --job-name={jobname}\n",
    "#SBATCH -A lband_group\n",
    "#SBATCH --mail-type=ALL\n",
    "cd {BASE_DIR}\n",
    "{RHESSYS_EXE} -st {st_year} {st_month} {st_day} {st_hour} -ed {ed_year} {ed_month} {ed_day} {ed_hour} -b -g -dynRtZoff -BGC_flag -fracDirectNdep 0.5 -gwtoriparian -t tecfiles/tec_daily_cali.txt -w worldfiles/worldfile -whdr {worldfile_hdr} -r flows/subflow.txt flows/surfflow.txt -pre {output_prefix} {param_str}\n",
    "\"\"\"\n",
    "        with open(job_path, \"w\") as f:\n",
    "            f.write(job_content)\n",
    "    print(f\"Created {NUM_DEFS_FOLDERS} job scripts in {JOB_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f11ed3-dd48-43e6-87dc-d9b131d549f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd35de-b42c-405d-8919-183ea6ae64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally run this code in the terminal \n",
    "#bash-4.4$cd /scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/1976_SA_1/salyan/model\n",
    "#python job_submission_3000.py ./SA_job .job \n",
    "\n",
    "#here the job_submission_3000.py will submit only 3000 jobs at a time and then again run the code \n",
    "\n",
    "#bash-4.4$cd /scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/1976_SA_1/salyan/model\n",
    "#python job_submission_6000.py ./SA_job .job \n",
    "#this will submit the next 3000 jobs and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ddb14-979c-4fea-b1e4-8db7617cec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55ffc9-2e09-4c07-bdf5-8ae5f5c438c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35982aab-b384-40fd-8353-45a2a6190490",
   "metadata": {},
   "source": [
    "###After all the simulations run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb7fdd-6fec-42e6-87e9-3b9e6f31e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of defs files again usinng this code given below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90e5c9-01eb-45f0-b0cc-348275d1aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "SA_DEFS_PARENT = os.path.join(BASE_DIR, \"SA_defs_morris\")\n",
    "NUM_DEFS_FOLDERS = len([d for d in os.listdir(SA_DEFS_PARENT) if d.startswith(\"defs\") and os.path.isdir(os.path.join(SA_DEFS_PARENT, d))])\n",
    "print(\"NUM_DEFS_FOLDERS =\", NUM_DEFS_FOLDERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e222469-2c0c-464f-9c47-108846892b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will convert the basin results into new csv file for each rin and save the data in the new folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee086f01-579b-4e22-bbfd-ffa7c2a04e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "DAILY_OUTDIR = os.path.join(BASE_DIR, \"SA_output_basin\")\n",
    "CSV_OUTDIR = os.path.join(BASE_DIR, \"SA_output_basin_csv\")  # New folder for CSVs\n",
    "os.makedirs(CSV_OUTDIR, exist_ok=True)\n",
    "\n",
    "# NUM_DEFS_FOLDERS should already be set before running this code\n",
    "\n",
    "# Variables of interest\n",
    "VARIABLES = ['day', 'month', 'year', 'streamflow', 'laiTREE', 'evap', 'trans']\n",
    "\n",
    "# === PROCESSING LOOP ===\n",
    "for i in range(1, NUM_DEFS_FOLDERS + 1):\n",
    "    result_file = os.path.join(DAILY_OUTDIR, f\"Salyan{i}_basin.daily\")\n",
    "    out_csv = os.path.join(CSV_OUTDIR, f\"basin{i}.csv\")\n",
    "    if not os.path.exists(result_file):\n",
    "        print(f\"WARNING: {result_file} not found! Skipping...\")\n",
    "        continue\n",
    "    # Read the header line\n",
    "    try:\n",
    "        with open(result_file, 'r') as f:\n",
    "            header = f.readline().strip().split()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read {result_file}: {e}\")\n",
    "        continue\n",
    "    # Try to read the data into a DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            result_file,\n",
    "            delim_whitespace=True,\n",
    "            comment=\"#\",\n",
    "            skiprows=1,\n",
    "            header=None,\n",
    "            names=header\n",
    "        )\n",
    "    except EmptyDataError:\n",
    "        print(f\"EmptyDataError: {result_file} is empty or malformed; skipped.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading {result_file}: {e}\")\n",
    "        continue\n",
    "    if df.empty:\n",
    "        print(f\"WARNING: {result_file} contains no data rows. Skipping...\")\n",
    "        continue\n",
    "    # Ensure required date components exist and are integers\n",
    "    date_missing = False\n",
    "    for col in ['year', 'month', 'day']:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].astype(int)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Could not convert column '{col}' in {result_file}: {e}\")\n",
    "                date_missing = True\n",
    "                break\n",
    "        else:\n",
    "            print(f\"WARNING: Column '{col}' missing in {result_file}. Skipping...\")\n",
    "            date_missing = True\n",
    "            break\n",
    "    if date_missing:\n",
    "        continue\n",
    "    # Build date column\n",
    "    df['date'] = pd.to_datetime(\n",
    "        df[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1),\n",
    "        format=\"%Y-%m-%d\",\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df = df.dropna(subset=['date'])\n",
    "    if df.empty:\n",
    "        print(f\"WARNING: All dates invalid in {result_file}. Skipping...\")\n",
    "        continue\n",
    "    # Extract needed columns safely\n",
    "    missing_vars = [v for v in ['streamflow', 'laiTREE', 'evap', 'trans'] if v not in df.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"WARNING: Missing variables {missing_vars} in {result_file}. Skipping...\")\n",
    "        continue\n",
    "    outdf = df[['date', 'streamflow', 'laiTREE', 'evap', 'trans']].copy()\n",
    "    outdf.loc[:, 'date'] = outdf['date'].dt.strftime(\"%Y-%m-%d\")\n",
    "    outdf.to_csv(out_csv, index=False)\n",
    "    print(f\"{out_csv} written.\")\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7263ffb-6a54-4370-ae17-61841c378b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95e5ed-d4ec-46b3-9f8e-7177827dd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code below will use the new generated csv files and calcualte the objective functions for each csv file and save the results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee0110-b4ed-423f-8cc9-312ca6387a5a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "SIM_DIR = os.path.join(BASE_DIR, \"SA_output_basin_csv\")  # Processed CSV outputs location\n",
    "OBS_CSV = os.path.join(BASE_DIR, \"W22_runoff.csv\")   # Observed runoff location\n",
    "RESULT_CSV = os.path.join(BASE_DIR, \"objective_function_result.csv\")  # Output results CSV\n",
    "\n",
    "# NUM_DEFS_FOLDERS must be defined before running this script!\n",
    "\n",
    "# === ANALYSIS DATE RANGE ===\n",
    "START_DATE = \"1976-01-01\"\n",
    "END_DATE   = \"2020-12-30\"\n",
    "\n",
    "# ---- Objective Functions ----\n",
    "def calc_nse(sim, obs):\n",
    "    obs_mean = np.mean(obs)\n",
    "    return 1 - np.sum((sim - obs)**2) / np.sum((obs - obs_mean)**2)\n",
    "\n",
    "def calc_rmse(sim, obs):\n",
    "    return np.sqrt(mean_squared_error(obs, sim))\n",
    "\n",
    "def calc_r2(sim, obs):\n",
    "    return r2_score(obs, sim)\n",
    "\n",
    "def calc_lognse(sim, obs):\n",
    "    mask = (sim > 0) & (obs > 0)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    logsim = np.log(sim[mask])\n",
    "    logobs = np.log(obs[mask])\n",
    "    return 1 - np.sum((logsim - logobs) ** 2) / np.sum((logobs - logobs.mean()) ** 2)\n",
    "\n",
    "# ---- LOAD OBSERVED DATA ----\n",
    "if not os.path.exists(OBS_CSV):\n",
    "    raise FileNotFoundError(f\"Observed data not found: {OBS_CSV}\")\n",
    "obs_df = pd.read_csv(OBS_CSV, parse_dates=[\"Date\"])\n",
    "obs_df.rename(columns={\"runoff(mm/day)_815km2\": \"runoff\"}, inplace=True)\n",
    "obs_df = obs_df[[\"Date\", \"runoff\"]]\n",
    "obs_df = obs_df[(obs_df[\"Date\"] >= START_DATE) & (obs_df[\"Date\"] <= END_DATE)]\n",
    "obs_df.set_index(\"Date\", inplace=True)\n",
    "years_with_obs = set(obs_df.dropna(subset=[\"runoff\"]).index.year)\n",
    "print(f\"Years with observed data: {sorted(years_with_obs)}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# ---- LOOP OVER SIMULATIONS ----\n",
    "for i in range(1, NUM_DEFS_FOLDERS + 1):\n",
    "    job_name = f\"basin{i}\"\n",
    "    defs_name = f\"defs{i}\"\n",
    "    sim_csv = os.path.join(SIM_DIR, f\"basin{i}.csv\")  # Correct file name!\n",
    "    if not os.path.exists(sim_csv):\n",
    "        print(f\"WARNING: {sim_csv} not found; skipping.\")\n",
    "        continue\n",
    "    try:\n",
    "        sim_df = pd.read_csv(sim_csv, parse_dates=[\"date\"])\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading {sim_csv}: {e}\")\n",
    "        continue\n",
    "    sim_df = sim_df[(sim_df[\"date\"] >= START_DATE) & (sim_df[\"date\"] <= END_DATE)]\n",
    "    sim_df.set_index(\"date\", inplace=True)\n",
    "    merged = pd.merge(sim_df, obs_df, left_index=True, right_index=True, how='inner')\n",
    "    if merged.empty:\n",
    "        print(f\"No overlap for {job_name}; skipping.\")\n",
    "        continue\n",
    "    merged = merged[merged.index.year.isin(years_with_obs)]\n",
    "    n_before = len(merged)\n",
    "    merged = merged.dropna(subset=[\"streamflow\", \"runoff\"])\n",
    "    dropped_n = n_before - len(merged)\n",
    "    if dropped_n > 0:\n",
    "        print(f\"{job_name}: Dropped {dropped_n} rows with NaN or no obs year.\")\n",
    "    if merged.empty:\n",
    "        print(f\"{job_name}: No valid rows remaining; skipping.\")\n",
    "        continue\n",
    "    sim = merged[\"streamflow\"].to_numpy()\n",
    "    obs = merged[\"runoff\"].to_numpy()\n",
    "    nse = calc_nse(sim, obs)\n",
    "    rmse = calc_rmse(sim, obs)\n",
    "    r2 = calc_r2(sim, obs)\n",
    "    lognse = calc_lognse(sim, obs)\n",
    "    results.append({\n",
    "        \"job\": job_name,\n",
    "        \"defs\": defs_name,\n",
    "        \"NSE\": nse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"logNSE\": lognse\n",
    "    })\n",
    "    print(f\"{job_name}: NSE={nse:.3f}, RMSE={rmse:.3f}, R2={r2:.3f}, logNSE={lognse:.3f}\")\n",
    "\n",
    "# ---- SAVE RESULTS ----\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULT_CSV, index=False)\n",
    "print(f\"\\nSaved results to {RESULT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20c422-9eb9-4b65-9407-3c6340c4f397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af77486-f09e-4140-a999-78ee3d2542ff",
   "metadata": {},
   "source": [
    "In this LAI-based optimization, the MODIS LAI values are first divided by 10 to convert them to the correct physical scale before comparison with the RHESSys simulated LAI. The R² and RMSE calculations are performed only on the dates where both MODIS LAI and RHESSys LAI values are available. Because MODIS LAI is not observed daily (due to 8-day compositing, cloud contamination, and missing retrievals), the code identifies and uses only the shared overlapping dates between both datasets and removes any missing values.\n",
    "\n",
    "If no overlapping dates exist for a given simulation, it is skipped.\n",
    "\n",
    "This approach ensures that the comparison is accurate, unbiased, and based solely on valid LAI observations, while correctly accounting for the MODIS LAI scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e59c3-44f6-45e7-8644-da8454e1e0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code will generate the optimization function using LAI for the dates that have MODIS LAI\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "SIM_DIR = os.path.join(BASE_DIR, \"SA_output_basin_csv\")  \n",
    "MODIS_LAI_CSV = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/LAI_w22.csv\"\n",
    "RESULT_CSV = os.path.join(BASE_DIR, \"objective_function_result_LAI.csv\") \n",
    "\n",
    "# NUM_DEFS_FOLDERS must already be defined\n",
    "\n",
    "# === ANALYSIS DATE RANGE ===\n",
    "START_DATE = \"2010-01-01\"\n",
    "END_DATE   = \"2012-12-31\"\n",
    "\n",
    "# ---- Objective Functions ----\n",
    "def calc_rmse(sim, obs):\n",
    "    return np.sqrt(mean_squared_error(obs, sim))\n",
    "\n",
    "def calc_r2(sim, obs):\n",
    "    return r2_score(obs, sim)\n",
    "\n",
    "# ---- LOAD MODIS LAI DATA ----\n",
    "if not os.path.exists(MODIS_LAI_CSV):\n",
    "    raise FileNotFoundError(f\"MODIS LAI data not found: {MODIS_LAI_CSV}\")\n",
    "\n",
    "modis_df = pd.read_csv(MODIS_LAI_CSV)\n",
    "\n",
    "if 'Date' not in modis_df.columns or 'Lai' not in modis_df.columns:\n",
    "    raise ValueError(f\"MODIS LAI CSV must contain 'Date' and 'Lai' columns. Found: {modis_df.columns.tolist()}\")\n",
    "\n",
    "modis_df['date'] = pd.to_datetime(modis_df['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "modis_df = modis_df.dropna(subset=['date'])\n",
    "modis_df = modis_df[['date', 'Lai']]\n",
    "\n",
    "# ✅ Divide MODIS LAI by 10 to convert to correct scale\n",
    "modis_df['Lai'] = modis_df['Lai'] / 10.0\n",
    "\n",
    "modis_df = modis_df[(modis_df['date'] >= START_DATE) & (modis_df['date'] <= END_DATE)]\n",
    "modis_df.set_index('date', inplace=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# ---- LOOP OVER SIMULATIONS ----\n",
    "for i in range(1, NUM_DEFS_FOLDERS + 1):\n",
    "\n",
    "    job_name = f\"basin{i}\"\n",
    "    defs_name = f\"defs{i}\"\n",
    "    sim_csv = os.path.join(SIM_DIR, f\"basin{i}.csv\")\n",
    "\n",
    "    if not os.path.exists(sim_csv):\n",
    "        print(f\"WARNING: {sim_csv} not found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        sim_df = pd.read_csv(sim_csv, parse_dates=[\"date\"])\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading {sim_csv}: {e}\")\n",
    "        continue\n",
    "\n",
    "    sim_df = sim_df[(sim_df[\"date\"] >= START_DATE) & (sim_df[\"date\"] <= END_DATE)]\n",
    "    sim_df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    if \"laiTREE\" not in sim_df.columns:\n",
    "        print(f\"{job_name}: 'laiTREE' missing; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ---- Match LAI Dates ----\n",
    "    shared_dates = sim_df.index.intersection(modis_df.index)\n",
    "    sim_lai = sim_df.loc[shared_dates, \"laiTREE\"].dropna()\n",
    "    modis_lai = modis_df.loc[shared_dates, \"Lai\"].dropna()\n",
    "\n",
    "    final_dates = sim_lai.index.intersection(modis_lai.index)\n",
    "    sim_lai = sim_lai.loc[final_dates]\n",
    "    modis_lai = modis_lai.loc[final_dates]\n",
    "\n",
    "    if len(sim_lai) == 0:\n",
    "        print(f\"{job_name}: No overlapping LAI dates; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ---- Compute Optimization Metrics ----\n",
    "    rmse = calc_rmse(sim_lai, modis_lai)\n",
    "    r2 = calc_r2(sim_lai, modis_lai)\n",
    "\n",
    "    results.append({\n",
    "        \"job\": job_name,\n",
    "        \"defs\": defs_name,\n",
    "        \"RMSE_LAI\": rmse,\n",
    "        \"R2_LAI\": r2,\n",
    "        \"n_dates\": len(final_dates)\n",
    "    })\n",
    "\n",
    "    print(f\"{job_name}: RMSE_LAI={rmse:.3f}, R2_LAI={r2:.3f}, n_dates={len(final_dates)}\")\n",
    "\n",
    "# ---- SAVE RESULTS ----\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nSaved results to {RESULT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301e8f1-9c91-4912-84be-d4d2a35d788b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6f89f-bdd7-4a40-a802-672b6fd7d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code below check if the simulation has nse value greater than the defined value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2458e7-dd25-4d87-9e9f-5cee8efa5753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Filter and show models with NSE > threshold ---\n",
    "NSE_THRESHOLD = 0.5 #for this, this is logNSE\n",
    "\n",
    "# Use the same result directory as before\n",
    "RESULT_DIR = BASE_DIR  # Saving in the base model directory\n",
    "\n",
    "good_nse_df = df[df[\"logNSE\"] > NSE_THRESHOLD].sort_values(by=\"logNSE\", ascending=False)\n",
    "\n",
    "if not good_nse_df.empty:\n",
    "    print(f\"\\nBasin models with NSE > {NSE_THRESHOLD}:\")\n",
    "    print(good_nse_df.to_string(index=False))\n",
    "    # Save filtered results for later inspection\n",
    "    good_nse_csv = os.path.join(RESULT_DIR, f\"nse_above_{NSE_THRESHOLD}.csv\")\n",
    "    good_nse_df.to_csv(good_nse_csv, index=False)\n",
    "    print(f\"\\nSaved filtered results: {good_nse_csv}\")\n",
    "else:\n",
    "    print(f\"\\nNo basins have NSE > {NSE_THRESHOLD}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8a9f0-de74-4b6e-96d8-92a717ff79df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251a820-a538-498e-ac41-1baba908b574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40016240-b9ad-4025-b30f-5ac250a7497c",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis (Morris-OAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2eb920-aef5-4575-a3a8-ef9082cdc39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ab9-db71-4ffa-a594-db78df5d3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code to join annd relates the def parameters files with the obbjective function csv file for all the simulations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82348a1f-22b0-47fa-8e53-91e70433ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "OBJECTIVE_FUNC_CSV = os.path.join(BASE_DIR, \"objective_function_result.csv\")\n",
    "DEFS_PARAM_CSV = os.path.join(BASE_DIR, \"defs_parameter_mapping.csv\")\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, \"All_Parameters_with_Objective_functions.csv\")\n",
    "\n",
    "# Load CSVs\n",
    "obj_df = pd.read_csv(OBJECTIVE_FUNC_CSV)\n",
    "defs_df = pd.read_csv(DEFS_PARAM_CSV)\n",
    "\n",
    "# Ensure key columns exist and match\n",
    "if \"defs\" not in obj_df.columns:\n",
    "    raise ValueError(\"'defs' column not found in objective function CSV!\")\n",
    "if \"defs_set\" not in defs_df.columns:\n",
    "    raise ValueError(\"'defs_set' column not found in defs parameter mapping CSV!\")\n",
    "\n",
    "# Merge on the corresponding columns\n",
    "merged_df = pd.merge(obj_df, defs_df, left_on=\"defs\", right_on=\"defs_set\", how=\"inner\")\n",
    "\n",
    "# Optionally, drop the duplicate 'defs_set' column if not needed:\n",
    "merged_df = merged_df.drop(columns=[\"defs_set\"])\n",
    "\n",
    "# Save new CSV\n",
    "merged_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Merged table written to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62951425-24cf-4901-9800-464a2da21797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf47278-efa2-43b8-901a-e36ec2785029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdf348-9a5e-4fa4-be3a-dea0e4bc4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Morris sensitivity analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b19b36-296b-4efe-840c-a8005d3c197a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from SALib.analyze import morris as morris_analyze\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIG ---\n",
    "BASE_DIR = \"/scratch/hjh7hp/Watershed_22_2025_fall/Watershed22_with_new_summer/Sharadha_khola_watershed/SA_full_range/1976/salyan/model\"\n",
    "PARAM_BOUNDS_CSV = os.path.join(BASE_DIR, \"Parameters_range_values.csv\")\n",
    "INPUT_CSV = os.path.join(BASE_DIR, \"All_Parameters_with_Objective_functions.csv\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"Morris_results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "TOP_N = 15\n",
    "TOP_N_DIR = os.path.join(RESULTS_DIR, f'top_{TOP_N}')\n",
    "os.makedirs(TOP_N_DIR, exist_ok=True)\n",
    "METRICS = ['NSE', 'RMSE', 'R2', 'logNSE']\n",
    "EXCLUDE_COLS = set(['job','defs','NSE','RMSE','R2','logNSE'])\n",
    "\n",
    "def wrap_label(label, width=12):\n",
    "    parts = label.split('_')\n",
    "    wrapped = []\n",
    "    for part in parts:\n",
    "        while len(part) > width:\n",
    "            wrapped.append(part[:width])\n",
    "            part = part[width:]\n",
    "        wrapped.append(part)\n",
    "    return '\\n'.join(wrapped)\n",
    "\n",
    "print(\"Loading data and filtering parameter columns...\")\n",
    "metrics_df = pd.read_csv(INPUT_CSV)\n",
    "param_bounds_df = pd.read_csv(PARAM_BOUNDS_CSV)\n",
    "\n",
    "all_potential_param_cols = []\n",
    "for col in param_bounds_df['Parameter name']:\n",
    "    if col not in EXCLUDE_COLS and col in metrics_df.columns:\n",
    "        try:\n",
    "            metrics_df[col].astype(float)\n",
    "            all_potential_param_cols.append(col)\n",
    "        except Exception:\n",
    "            print(f\"Excluding parameter '{col}' (non-numeric values detected).\")\n",
    "numeric_param_cols = all_potential_param_cols\n",
    "\n",
    "print(f\"Number of numeric parameter columns to use: {len(numeric_param_cols)}\")\n",
    "\n",
    "# Parameter bounds dictionary\n",
    "used_param_bounds = {\n",
    "    p: tuple(param_bounds_df[param_bounds_df['Parameter name'] == p][['lower limit', 'upper limit']].values[0])\n",
    "    for p in numeric_param_cols\n",
    "}\n",
    "\n",
    "metrics_df_numeric = metrics_df.copy()\n",
    "for col in numeric_param_cols:\n",
    "    metrics_df_numeric[col] = pd.to_numeric(metrics_df_numeric[col], errors='coerce')\n",
    "\n",
    "valid_mask = metrics_df_numeric[METRICS].notna().any(axis=1)\n",
    "param_mask = ~metrics_df_numeric[numeric_param_cols].isna().any(axis=1)\n",
    "metrics_df_cleaned = metrics_df_numeric.loc[valid_mask & param_mask, :]\n",
    "\n",
    "# CORRECT HERE\n",
    "n_cleaned = int(metrics_df_cleaned.shape[0])\n",
    "if n_cleaned == 0:\n",
    "    print(\"No usable simulations remain after filtering for numeric parameter columns.\")\n",
    "    exit()\n",
    "print(f\"Rows usable for Morris analysis: {n_cleaned}\")\n",
    "\n",
    "traj_len = int(len(numeric_param_cols)) + 1\n",
    "n_traj = n_cleaned // traj_len\n",
    "n_expected = n_traj * traj_len\n",
    "df_morris = metrics_df_cleaned.iloc[:n_expected, :]\n",
    "print(f\"Analysis uses {df_morris.shape[0]} rows, {n_traj} full trajectories.\")\n",
    "\n",
    "problem = {\n",
    "    'num_vars': len(numeric_param_cols),\n",
    "    'names': numeric_param_cols,\n",
    "    'bounds': [used_param_bounds[p] for p in numeric_param_cols]\n",
    "}\n",
    "results_tables = []\n",
    "top_results_tables = []\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, metric_col in enumerate(METRICS):\n",
    "    if metric_col not in df_morris.columns:\n",
    "        print(f\"Metric '{metric_col}' not found. Skipping.\")\n",
    "        continue\n",
    "    mask_metric = ~df_morris[metric_col].isna()\n",
    "    X = df_morris.loc[mask_metric, numeric_param_cols].values.astype(float)\n",
    "    Y = df_morris.loc[mask_metric, metric_col].values.astype(float)\n",
    "    n_metric = X.shape[0]\n",
    "    traj_metric = n_metric // traj_len\n",
    "    n_metric_expected = traj_metric * traj_len\n",
    "    if traj_metric == 0:\n",
    "        print(f\"Not enough complete trajectories for {metric_col}. Skipping.\")\n",
    "        continue\n",
    "    X = X[:n_metric_expected, :]\n",
    "    Y = Y[:n_metric_expected]\n",
    "    num_levels = 4\n",
    "    results = morris_analyze.analyze(\n",
    "        problem,\n",
    "        X,\n",
    "        Y,\n",
    "        num_levels=num_levels,\n",
    "        print_to_console=False\n",
    "    )\n",
    "    results_df = pd.DataFrame({\n",
    "        'Parameter': problem['names'],\n",
    "        'mu_star': results['mu_star'],\n",
    "        'sigma': results['sigma'],\n",
    "        'mu': results['mu'],\n",
    "        'mu_star_conf': results['mu_star_conf'],\n",
    "        'metric': metric_col,\n",
    "        'lower_limit': [problem['bounds'][i][0] for i in range(len(problem['names']))],\n",
    "        'upper_limit': [problem['bounds'][i][1] for i in range(len(problem['names']))]\n",
    "    }).sort_values('mu_star', ascending=False).reset_index(drop=True)\n",
    "    results_tables.append(results_df)\n",
    "    metric_csv_all = os.path.join(RESULTS_DIR, f'morris_sensitivity_{metric_col}_all.csv')\n",
    "    results_df.to_csv(metric_csv_all, index=False)\n",
    "    top_df = results_df.iloc[:TOP_N, :].copy()\n",
    "    top_results_tables.append(top_df)\n",
    "    metric_csv_top = os.path.join(TOP_N_DIR, f'top_{TOP_N}_morris_sensitivity_{metric_col}.csv')\n",
    "    top_df.to_csv(metric_csv_top, index=False)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    params_sorted = [wrap_label(p, 12) for p in top_df['Parameter']]\n",
    "    mu_star_sorted = top_df['mu_star']\n",
    "    bars = plt.bar(range(len(params_sorted)), mu_star_sorted, color=colors[idx % len(colors)], edgecolor='black', alpha=0.85)\n",
    "    plt.ylabel('Morris μ*', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top {TOP_N} Morris Sensitivity ({metric_col})', fontsize=15, fontweight='bold', pad=12)\n",
    "    plt.xticks(range(len(params_sorted)), params_sorted, rotation=0, ha='center', fontsize=8, color='black')\n",
    "    for idx_bar, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f\"{height:.2f}\", xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 4), textcoords=\"offset points\", ha='center', va='bottom', fontsize=11)\n",
    "    plt.xlabel('Parameter (ranked)', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "    plot_path = os.path.join(TOP_N_DIR, f'top_{TOP_N}_morris_{metric_col}.png')\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "if results_tables:\n",
    "    results_all = pd.concat(results_tables, ignore_index=True)\n",
    "    results_all.to_csv(os.path.join(RESULTS_DIR, f'morris_sensitivity_all_metrics_ranked.csv'), index=False)\n",
    "if top_results_tables:\n",
    "    top_results_all = pd.concat(top_results_tables, ignore_index=True)\n",
    "    top_results_all.to_csv(os.path.join(TOP_N_DIR, f'top_{TOP_N}_morris_sensitivity_all_metrics_ranked.csv'), index=False)\n",
    "print(f\"\\nTop {TOP_N} Morris sensitivity results and bar plots saved in:\\n {TOP_N_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464b4ad-e914-4b96-8fef-a4c488b534c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
